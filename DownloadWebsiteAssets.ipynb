{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1IinpmcflIPNBveOy3iBa5jC-93VG0mKE",
      "authorship_tag": "ABX9TyO2kVzy8FwK4W5NpiszfaC+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SabirDivs/Python-Scripts/blob/main/DownloadWebsiteAssets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0wkIIMgj7wo",
        "outputId": "93851472-33a5-4207-8956-677122c17a8e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tl8Oa07PjWxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ecb9e1e-fa41-4ffe-a76c-416cd31156ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab\n",
            "Enter website URL to download: https://theolivebranchpk.com/\n",
            "Enter output directory [downloaded_website]: /content/drive/MyDrive/PythonApps/WebsitesDownloader\n",
            "Processing page: https://theolivebranchpk.com/\n",
            "Downloaded: https://theolivebranchpk.com/images/logos/theOliveBranchPK-SVG.png => theolivebranchpk.com/_images_logos/theOliveBranchPK-SVG.png\n",
            "Downloaded: https://theolivebranchpk.com/ => theolivebranchpk.com/index.html\n",
            "Downloaded: https://theolivebranchpk.com/fonts/fontawesome-webfont.woff2 => theolivebranchpk.com/_fonts/fontawesome-webfont.woff2\n",
            "Downloaded: https://theolivebranchpk.com/css/assets.min.css => theolivebranchpk.com/_css/assets.min.css\n",
            "Downloaded: https://theolivebranchpk.com/css/fonts.css => theolivebranchpk.com/_css/fonts.css\n",
            "Downloaded: https://theolivebranchpk.com/css/styles.css => theolivebranchpk.com/_css/styles.css\n",
            "Downloaded: https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css => cdnjs.cloudflare.com/_ajax_libs_font-awesome_6.6.0_css/all.min.css.css\n",
            "Failed to download https://theolivebranchpk.com/public/js/website.assets.min.js: 404 Client Error: Not Found for url: https://theolivebranchpk.com/public/js/website.assets.min.js\n",
            "Downloaded: https://theolivebranchpk.com/js/website.assets.min.js => theolivebranchpk.com/_js/website.assets.min.js\n",
            "Downloaded: https://theolivebranchpk.com/js/website.min.js => theolivebranchpk.com/_js/website.min.js\n",
            "Downloaded: https://theolivebranchpk.com/images/Logo-Transparent_croped.png => theolivebranchpk.com/_images/Logo-Transparent_croped.png\n",
            "Downloaded: https://theolivebranchpk.com/images/Logo-Transparent-Simple_cropped.png => theolivebranchpk.com/_images/Logo-Transparent-Simple_cropped.png\n",
            "Failed to download https://theolivebranchpk.com/$: 404 Client Error: Not Found for url: https://theolivebranchpk.com/$\n",
            "Failed to download https://theolivebranchpk.com/text/html; charset=UTF-8: 404 Client Error: Not Found for url: https://theolivebranchpk.com/text/html;%20charset=UTF-8\n",
            "Failed to download https://theolivebranchpk.com/TheOliveBranch: 404 Client Error: Not Found for url: https://theolivebranchpk.com/TheOliveBranch\n",
            "Failed to download https://theolivebranchpk.com/website: 404 Client Error: Not Found for url: https://theolivebranchpk.com/website\n",
            "Failed to download https://theolivebranchpk.com/IE=Edge: 404 Client Error: Not Found for url: https://theolivebranchpk.com/IE=Edge\n",
            "Failed to download https://theolivebranchpk.com/width=device-width, initial-scale=1.0: 404 Client Error: Not Found for url: https://theolivebranchpk.com/width=device-width,%20initial-scale=1.0\n",
            "Downloaded: https://theolivebranchpk.com/images/mt-1923-slider-img01.jpg => theolivebranchpk.com/_images/mt-1923-slider-img01.jpg\n",
            "Downloaded: https://theolivebranchpk.com/images/mt-1923-slider-img02.png => theolivebranchpk.com/_images/mt-1923-slider-img02.png\n",
            "Downloaded: https://theolivebranchpk.com/images/mt-1923-home-img02.jpg => theolivebranchpk.com/_images/mt-1923-home-img02.jpg\n",
            "Downloaded: https://theolivebranchpk.com/images/mt-1923-home-img03.jpg => theolivebranchpk.com/_images/mt-1923-home-img03.jpg\n",
            "Downloaded: https://theolivebranchpk.com/images/mt-1923-home-img04.jpg => theolivebranchpk.com/_images/mt-1923-home-img04.jpg\n",
            "Downloaded: https://theolivebranchpk.com/images/mt-1923-content-bg01.jpg => theolivebranchpk.com/_images/mt-1923-content-bg01.jpg\n",
            "Downloaded: https://theolivebranchpk.com/images/mt-1923-home-img06.jpg => theolivebranchpk.com/_images/mt-1923-home-img06.jpg\n",
            "Downloaded: https://theolivebranchpk.com/images/mt-1923-home-img07.jpg => theolivebranchpk.com/_images/mt-1923-home-img07.jpg\n",
            "Downloaded: https://theolivebranchpk.com/images/mt-1923-content-bg02.jpg => theolivebranchpk.com/_images/mt-1923-content-bg02.jpg\n",
            "Processing page: https://theolivebranchpk.com/about\n",
            "Failed to download https://theolivebranchpk.com/public/js/website.assets.min.js: 404 Client Error: Not Found for url: https://theolivebranchpk.com/public/js/website.assets.min.js\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/text/html; charset=UTF-8: 404 Client Error: Not Found for url: https://theolivebranchpk.com/text/html;%20charset=UTF-8\n",
            "Failed to download https://theolivebranchpk.com/TheOliveBranch: 404 Client Error: Not Found for url: https://theolivebranchpk.com/TheOliveBranch\n",
            "Failed to download https://theolivebranchpk.com/website: 404 Client Error: Not Found for url: https://theolivebranchpk.com/website\n",
            "Failed to download https://theolivebranchpk.com/IE=Edge: 404 Client Error: Not Found for url: https://theolivebranchpk.com/IE=Edge\n",
            "Failed to download https://theolivebranchpk.com/width=device-width, initial-scale=1.0: 404 Client Error: Not Found for url: https://theolivebranchpk.com/width=device-width,%20initial-scale=1.0\n",
            "Downloaded: https://theolivebranchpk.com/images/mt-1923-content-bg03.jpg => theolivebranchpk.com/_images/mt-1923-content-bg03.jpg\n",
            "Failed to download https://theolivebranchpk.com/images/mt-1923-content-bg04.jpg: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/mt-1923-content-bg04.jpg\n",
            "Downloaded: https://theolivebranchpk.com/images/mt-1923-content-bg05.jpg => theolivebranchpk.com/_images/mt-1923-content-bg05.jpg\n",
            "Processing page: https://theolivebranchpk.com/products\n",
            "Failed to download https://theolivebranchpk.com/public/js/website.assets.min.js: 404 Client Error: Not Found for url: https://theolivebranchpk.com/public/js/website.assets.min.js\n",
            "Downloaded: https://theolivebranchpk.com/images/mt-1923-products-icons01.png => theolivebranchpk.com/_images/mt-1923-products-icons01.png\n",
            "Downloaded: https://theolivebranchpk.com/images/mt-1923-products-icons02.png => theolivebranchpk.com/_images/mt-1923-products-icons02.png\n",
            "Downloaded: https://theolivebranchpk.com/images/mt-1923-products-icons03.png => theolivebranchpk.com/_images/mt-1923-products-icons03.png\n",
            "Downloaded: https://theolivebranchpk.com/images/mt-1923-products-icons04.png => theolivebranchpk.com/_images/mt-1923-products-icons04.png\n",
            "Failed to download https://theolivebranchpk.com/images/uploads/Olive Hair Growth Oil-1.jpeg: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/uploads/Olive%20Hair%20Growth%20Oil-1.jpeg\n",
            "Downloaded: https://theolivebranchpk.com/images/uploads/Products/cracked-heels-balm/IMG_1610639176047656017 (1) (1).png => theolivebranchpk.com/_images_uploads_Products_cracked-heels-balm/IMG_1610639176047656017 (1) (1).png\n",
            "Downloaded: https://theolivebranchpk.com/images/uploads/Products/olive-face-serum/IMG_2692211137873225470 (2).png => theolivebranchpk.com/_images_uploads_Products_olive-face-serum/IMG_2692211137873225470 (2).png\n",
            "Failed to download https://theolivebranchpk.com/images/uploads/Olive Lip Balm-1.jpeg: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/uploads/Olive%20Lip%20Balm-1.jpeg\n",
            "Downloaded: https://theolivebranchpk.com/images/uploads/Products/olive-oil-moisturizer/IMG_4545634337358454259 (2).png => theolivebranchpk.com/_images_uploads_Products_olive-oil-moisturizer/IMG_4545634337358454259 (2).png\n",
            "Downloaded: https://theolivebranchpk.com/images/uploads/Products/olive-sugar-scrub/IMG_1070212687240903344 (3).png => theolivebranchpk.com/_images_uploads_Products_olive-sugar-scrub/IMG_1070212687240903344 (3).png\n",
            "Downloaded: https://theolivebranchpk.com/images/uploads/Products/olive-face-mask/IMG_8060440238595624455 (1) (1).png => theolivebranchpk.com/_images_uploads_Products_olive-face-mask/IMG_8060440238595624455 (1) (1).png\n",
            "Downloaded: https://theolivebranchpk.com/images/uploads/Products/extra-virgin-olive-oil/IMG_7371056640588454646 (4).png => theolivebranchpk.com/_images_uploads_Products_extra-virgin-olive-oil/IMG_7371056640588454646 (4).png\n",
            "Failed to download https://theolivebranchpk.com/text/html; charset=UTF-8: 404 Client Error: Not Found for url: https://theolivebranchpk.com/text/html;%20charset=UTF-8\n",
            "Failed to download https://theolivebranchpk.com/TheOliveBranch: 404 Client Error: Not Found for url: https://theolivebranchpk.com/TheOliveBranch\n",
            "Failed to download https://theolivebranchpk.com/website: 404 Client Error: Not Found for url: https://theolivebranchpk.com/website\n",
            "Failed to download https://theolivebranchpk.com/IE=Edge: 404 Client Error: Not Found for url: https://theolivebranchpk.com/IE=Edge\n",
            "Failed to download https://theolivebranchpk.com/width=device-width, initial-scale=1.0: 404 Client Error: Not Found for url: https://theolivebranchpk.com/width=device-width,%20initial-scale=1.0\n",
            "Failed to download https://theolivebranchpk.com/images/mt-1923-content-bg04.jpg: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/mt-1923-content-bg04.jpg\n",
            "Failed to download https://theolivebranchpk.com/images/mt-1923-content-bg06.jpg: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/mt-1923-content-bg06.jpg\n",
            "Downloaded: https://theolivebranchpk.com/images/mt-1923-content-bg07.jpg => theolivebranchpk.com/_images/mt-1923-content-bg07.jpg\n",
            "Processing page: https://theolivebranchpk.com/singleProduct\n",
            "Failed to download https://theolivebranchpk.com/public/js/website.assets.min.js: 404 Client Error: Not Found for url: https://theolivebranchpk.com/public/js/website.assets.min.js\n",
            "Failed to download https://theolivebranchpk.com/images/uploads/Olive Hair Growth Oil-1.jpeg: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/uploads/Olive%20Hair%20Growth%20Oil-1.jpeg\n",
            "Failed to download https://theolivebranchpk.com/images/uploads/Olive Lip Balm-1.jpeg: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/uploads/Olive%20Lip%20Balm-1.jpeg\n",
            "Failed to download https://theolivebranchpk.com/text/html; charset=UTF-8: 404 Client Error: Not Found for url: https://theolivebranchpk.com/text/html;%20charset=UTF-8\n",
            "Failed to download https://theolivebranchpk.com/TheOliveBranch: 404 Client Error: Not Found for url: https://theolivebranchpk.com/TheOliveBranch\n",
            "Failed to download https://theolivebranchpk.com/website: 404 Client Error: Not Found for url: https://theolivebranchpk.com/website\n",
            "Failed to download https://theolivebranchpk.com/IE=Edge: 404 Client Error: Not Found for url: https://theolivebranchpk.com/IE=Edge\n",
            "Failed to download https://theolivebranchpk.com/width=device-width, initial-scale=1.0: 404 Client Error: Not Found for url: https://theolivebranchpk.com/width=device-width,%20initial-scale=1.0\n",
            "Failed to download https://theolivebranchpk.com/images/mt-1923-content-bg04.jpg: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/mt-1923-content-bg04.jpg\n",
            "Processing page: https://theolivebranchpk.com/partnership\n",
            "Failed to download https://theolivebranchpk.com/public/js/website.assets.min.js: 404 Client Error: Not Found for url: https://theolivebranchpk.com/public/js/website.assets.min.js\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/text/html; charset=UTF-8: 404 Client Error: Not Found for url: https://theolivebranchpk.com/text/html;%20charset=UTF-8\n",
            "Failed to download https://theolivebranchpk.com/TheOliveBranch: 404 Client Error: Not Found for url: https://theolivebranchpk.com/TheOliveBranch\n",
            "Failed to download https://theolivebranchpk.com/website: 404 Client Error: Not Found for url: https://theolivebranchpk.com/website\n",
            "Failed to download https://theolivebranchpk.com/IE=Edge: 404 Client Error: Not Found for url: https://theolivebranchpk.com/IE=Edge\n",
            "Failed to download https://theolivebranchpk.com/width=device-width, initial-scale=1.0: 404 Client Error: Not Found for url: https://theolivebranchpk.com/width=device-width,%20initial-scale=1.0\n",
            "Failed to download https://theolivebranchpk.com/images/mt-1923-content-bg04.jpg: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/mt-1923-content-bg04.jpg\n",
            "Processing page: https://theolivebranchpk.com/storeLocation\n",
            "Failed to download https://theolivebranchpk.com/public/js/website.assets.min.js: 404 Client Error: Not Found for url: https://theolivebranchpk.com/public/js/website.assets.min.js\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/text/html; charset=UTF-8: 404 Client Error: Not Found for url: https://theolivebranchpk.com/text/html;%20charset=UTF-8\n",
            "Failed to download https://theolivebranchpk.com/TheOliveBranch: 404 Client Error: Not Found for url: https://theolivebranchpk.com/TheOliveBranch\n",
            "Failed to download https://theolivebranchpk.com/website: 404 Client Error: Not Found for url: https://theolivebranchpk.com/website\n",
            "Failed to download https://theolivebranchpk.com/IE=Edge: 404 Client Error: Not Found for url: https://theolivebranchpk.com/IE=Edge\n",
            "Failed to download https://theolivebranchpk.com/width=device-width, initial-scale=1.0: 404 Client Error: Not Found for url: https://theolivebranchpk.com/width=device-width,%20initial-scale=1.0\n",
            "Processing page: https://theolivebranchpk.com/gallery\n",
            "Failed to download https://theolivebranchpk.com/public/js/website.assets.min.js: 404 Client Error: Not Found for url: https://theolivebranchpk.com/public/js/website.assets.min.js\n",
            "Failed to download https://theolivebranchpk.com/text/html; charset=UTF-8: 404 Client Error: Not Found for url: https://theolivebranchpk.com/text/html;%20charset=UTF-8\n",
            "Failed to download https://theolivebranchpk.com/TheOliveBranch: 404 Client Error: Not Found for url: https://theolivebranchpk.com/TheOliveBranch\n",
            "Failed to download https://theolivebranchpk.com/website: 404 Client Error: Not Found for url: https://theolivebranchpk.com/website\n",
            "Failed to download https://theolivebranchpk.com/IE=Edge: 404 Client Error: Not Found for url: https://theolivebranchpk.com/IE=Edge\n",
            "Failed to download https://theolivebranchpk.com/width=device-width, initial-scale=1.0: 404 Client Error: Not Found for url: https://theolivebranchpk.com/width=device-width,%20initial-scale=1.0\n",
            "Failed to download https://theolivebranchpk.com/images/mt-1923-content-bg08.jpg: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/mt-1923-content-bg08.jpg\n",
            "Processing page: https://theolivebranchpk.com/contact\n",
            "Failed to download https://theolivebranchpk.com/public/js/website.assets.min.js: 404 Client Error: Not Found for url: https://theolivebranchpk.com/public/js/website.assets.min.js\n",
            "Failed to download https://theolivebranchpk.com/text/html; charset=UTF-8: 404 Client Error: Not Found for url: https://theolivebranchpk.com/text/html;%20charset=UTF-8\n",
            "Failed to download https://theolivebranchpk.com/TheOliveBranch: 404 Client Error: Not Found for url: https://theolivebranchpk.com/TheOliveBranch\n",
            "Failed to download https://theolivebranchpk.com/website: 404 Client Error: Not Found for url: https://theolivebranchpk.com/website\n",
            "Failed to download https://theolivebranchpk.com/IE=Edge: 404 Client Error: Not Found for url: https://theolivebranchpk.com/IE=Edge\n",
            "Failed to download https://theolivebranchpk.com/width=device-width, initial-scale=1.0: 404 Client Error: Not Found for url: https://theolivebranchpk.com/width=device-width,%20initial-scale=1.0\n",
            "Failed to download https://theolivebranchpk.com/images/mt-1923-contacts-icon01.png: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/mt-1923-contacts-icon01.png\n",
            "Failed to download https://theolivebranchpk.com/images/mt-1923-contacts-icon03.png: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/mt-1923-contacts-icon03.png\n",
            "Failed to download https://theolivebranchpk.com/images/mt-1923-contacts-icon04.png: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/mt-1923-contacts-icon04.png\n",
            "Processing page: https://theolivebranchpk.com/privacy-policy.html\n",
            "Failed to process page https://theolivebranchpk.com/privacy-policy.html: 404 Client Error: Not Found for url: https://theolivebranchpk.com/privacy-policy.html\n",
            "Processing page: https://theolivebranchpk.com/contact/submit\n",
            "Failed to process page https://theolivebranchpk.com/contact/submit: 404 Client Error: Not Found for url: https://theolivebranchpk.com/contact/submit\n",
            "Processing page: https://theolivebranchpk.com/mt-1923-gallery-img01bg.jpg.html\n",
            "Failed to process page https://theolivebranchpk.com/mt-1923-gallery-img01bg.jpg.html: 404 Client Error: Not Found for url: https://theolivebranchpk.com/mt-1923-gallery-img01bg.jpg.html\n",
            "Processing page: https://theolivebranchpk.com/mt-1923-gallery-img02bg.jpg.html\n",
            "Failed to process page https://theolivebranchpk.com/mt-1923-gallery-img02bg.jpg.html: 404 Client Error: Not Found for url: https://theolivebranchpk.com/mt-1923-gallery-img02bg.jpg.html\n",
            "Processing page: https://theolivebranchpk.com/mt-1923-gallery-img03bg.jpg.html\n",
            "Failed to process page https://theolivebranchpk.com/mt-1923-gallery-img03bg.jpg.html: 404 Client Error: Not Found for url: https://theolivebranchpk.com/mt-1923-gallery-img03bg.jpg.html\n",
            "Processing page: https://theolivebranchpk.com/mt-1923-gallery-img04bg.jpg.html\n",
            "Failed to process page https://theolivebranchpk.com/mt-1923-gallery-img04bg.jpg.html: 404 Client Error: Not Found for url: https://theolivebranchpk.com/mt-1923-gallery-img04bg.jpg.html\n",
            "Processing page: https://theolivebranchpk.com/mt-1923-gallery-img05.jpg.html\n",
            "Failed to process page https://theolivebranchpk.com/mt-1923-gallery-img05.jpg.html: 404 Client Error: Not Found for url: https://theolivebranchpk.com/mt-1923-gallery-img05.jpg.html\n",
            "Processing page: https://theolivebranchpk.com/mt-1923-gallery-img06bg.jpg.html\n",
            "Failed to process page https://theolivebranchpk.com/mt-1923-gallery-img06bg.jpg.html: 404 Client Error: Not Found for url: https://theolivebranchpk.com/mt-1923-gallery-img06bg.jpg.html\n",
            "Processing page: https://theolivebranchpk.com/mt-1923-gallery-img07bg.jpg.html\n",
            "Failed to process page https://theolivebranchpk.com/mt-1923-gallery-img07bg.jpg.html: 404 Client Error: Not Found for url: https://theolivebranchpk.com/mt-1923-gallery-img07bg.jpg.html\n",
            "Processing page: https://theolivebranchpk.com/mt-1923-gallery-img08bg.jpg.html\n",
            "Failed to process page https://theolivebranchpk.com/mt-1923-gallery-img08bg.jpg.html: 404 Client Error: Not Found for url: https://theolivebranchpk.com/mt-1923-gallery-img08bg.jpg.html\n",
            "Processing page: https://theolivebranchpk.com/mt-1923-gallery-img09bg.jpg.html\n",
            "Failed to process page https://theolivebranchpk.com/mt-1923-gallery-img09bg.jpg.html: 404 Client Error: Not Found for url: https://theolivebranchpk.com/mt-1923-gallery-img09bg.jpg.html\n",
            "Processing page: https://theolivebranchpk.com/cart/add\n",
            "Failed to process page https://theolivebranchpk.com/cart/add: 404 Client Error: Not Found for url: https://theolivebranchpk.com/cart/add\n",
            "Processing page: https://theolivebranchpk.com/products.html\n",
            "Failed to process page https://theolivebranchpk.com/products.html: 404 Client Error: Not Found for url: https://theolivebranchpk.com/products.html\n",
            "Processing page: https://theolivebranchpk.com/about/\n",
            "Downloaded: https://theolivebranchpk.com/about/ => theolivebranchpk.com/_about/index.html\n",
            "Failed to download https://theolivebranchpk.com/public/js/website.assets.min.js: 404 Client Error: Not Found for url: https://theolivebranchpk.com/public/js/website.assets.min.js\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/images/svg%3E: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/svg%3E\n",
            "Failed to download https://theolivebranchpk.com/about/text/html; charset=UTF-8: 404 Client Error: Not Found for url: https://theolivebranchpk.com/about/text/html;%20charset=UTF-8\n",
            "Failed to download https://theolivebranchpk.com/about/TheOliveBranch: 404 Client Error: Not Found for url: https://theolivebranchpk.com/about/TheOliveBranch\n",
            "Failed to download https://theolivebranchpk.com/about/website: 404 Client Error: Not Found for url: https://theolivebranchpk.com/about/website\n",
            "Failed to download https://theolivebranchpk.com/about/IE=Edge: 404 Client Error: Not Found for url: https://theolivebranchpk.com/about/IE=Edge\n",
            "Failed to download https://theolivebranchpk.com/about/width=device-width, initial-scale=1.0: 404 Client Error: Not Found for url: https://theolivebranchpk.com/about/width=device-width,%20initial-scale=1.0\n",
            "Failed to download https://theolivebranchpk.com/images/mt-1923-content-bg04.jpg: 404 Client Error: Not Found for url: https://theolivebranchpk.com/images/mt-1923-content-bg04.jpg\n",
            "\n",
            "Website successfully downloaded to: /content/drive/MyDrive/PythonApps/WebsitesDownloader\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import requests\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from bs4 import BeautifulSoup\n",
        "import mimetypes\n",
        "\n",
        "class WebsiteDownloader:\n",
        "    def __init__(self, base_url, output_dir):\n",
        "        self.base_url = base_url\n",
        "        self.output_dir = os.path.abspath(output_dir)\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Connection': 'keep-alive',\n",
        "        })\n",
        "        self.visited_urls = set()\n",
        "        self.asset_map = {}\n",
        "        self.base_domain = urlparse(base_url).netloc\n",
        "\n",
        "    def sanitize_path(self, path):\n",
        "        \"\"\"Create safe filesystem paths from URLs\"\"\"\n",
        "        path = re.sub(r'[^\\w\\-_\\.]', '_', path)\n",
        "        return path.lstrip('/')\n",
        "\n",
        "    def get_resource_path(self, url, content_type=None):\n",
        "        \"\"\"Generate local path for a resource\"\"\"\n",
        "        parsed = urlparse(url)\n",
        "        path = parsed.path or '/index.html'\n",
        "\n",
        "        if path.endswith('/'):\n",
        "            path += 'index.html'\n",
        "\n",
        "        # Determine filename with extension\n",
        "        filename = os.path.basename(path)\n",
        "        if '.' not in filename or '.' in urlparse(url).path.split('/')[-1][:5]:\n",
        "            if content_type:\n",
        "                ext = mimetypes.guess_extension(content_type.split(';')[0]) or ''\n",
        "                filename = f\"{filename}{ext}\" if filename else f\"resource{ext}\"\n",
        "            else:\n",
        "                filename = filename or \"resource\"\n",
        "\n",
        "        domain_path = self.sanitize_path(parsed.netloc)\n",
        "        dir_path = os.path.dirname(path)\n",
        "        if dir_path != '/':\n",
        "            safe_dir = self.sanitize_path(dir_path)\n",
        "            resource_path = os.path.join(domain_path, safe_dir, filename)\n",
        "        else:\n",
        "            resource_path = os.path.join(domain_path, filename)\n",
        "\n",
        "        return resource_path\n",
        "\n",
        "    def download_resource(self, url, referer=None):\n",
        "        \"\"\"Download resource and return its local path\"\"\"\n",
        "        if url in self.asset_map:\n",
        "            return self.asset_map[url]\n",
        "\n",
        "        try:\n",
        "            # Handle relative URLs\n",
        "            if not urlparse(url).scheme:\n",
        "                url = urljoin(self.base_url, url)\n",
        "\n",
        "            headers = {'Referer': referer} if referer else {}\n",
        "            response = self.session.get(url, headers=headers, timeout=10, stream=True)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Determine content type\n",
        "            content_type = response.headers.get('Content-Type', '').split(';')[0]\n",
        "            if not content_type:\n",
        "                content_type = mimetypes.guess_type(url)[0] or 'application/octet-stream'\n",
        "\n",
        "            # Generate local path\n",
        "            local_relative_path = self.get_resource_path(url, content_type)\n",
        "            local_path = os.path.join(self.output_dir, local_relative_path)\n",
        "\n",
        "            # Ensure directory exists\n",
        "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "\n",
        "            # Save content\n",
        "            with open(local_path, 'wb') as f:\n",
        "                for chunk in response.iter_content(8192):\n",
        "                    f.write(chunk)\n",
        "\n",
        "            self.asset_map[url] = local_relative_path\n",
        "            print(f\"Downloaded: {url} => {local_relative_path}\")\n",
        "            return local_relative_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to download {url}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def process_css(self, content, css_url):\n",
        "        \"\"\"Process CSS content and download nested resources\"\"\"\n",
        "        def replace_url(match):\n",
        "            url = match.group(1).strip('\\'\"')\n",
        "            if url.startswith(('data:', 'http:', 'https:')):\n",
        "                return match.group(0)\n",
        "\n",
        "            abs_url = urljoin(css_url, url)\n",
        "            local_path = self.download_resource(abs_url, referer=css_url)\n",
        "            return f\"url('{local_path}')\" if local_path else match.group(0)\n",
        "\n",
        "        return re.sub(r'url\\([\\'\"]?(.*?)[\\'\"]?\\)', replace_url, content)\n",
        "\n",
        "    def process_html(self, content, page_url):\n",
        "        \"\"\"Process HTML content and download linked resources\"\"\"\n",
        "        try:\n",
        "            soup = BeautifulSoup(content, 'html.parser')\n",
        "        except:\n",
        "            soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "        # Update resource tags\n",
        "        for tag, attr in [\n",
        "            ('link', 'href'),\n",
        "            ('script', 'src'),\n",
        "            ('img', 'src'),\n",
        "            ('source', 'src'),\n",
        "            ('audio', 'src'),\n",
        "            ('video', 'src'),\n",
        "            ('iframe', 'src'),\n",
        "            ('embed', 'src'),\n",
        "            ('object', 'data'),\n",
        "            ('meta', 'content'),\n",
        "            ('img', 'srcset')\n",
        "        ]:\n",
        "            for element in soup.find_all(tag, **{attr: True}):\n",
        "                urls = []\n",
        "                if attr == 'srcset':\n",
        "                    # Handle srcset with multiple URLs\n",
        "                    srcset = element[attr]\n",
        "                    parts = [p.strip() for p in srcset.split(',') if p.strip()]\n",
        "                    for part in parts:\n",
        "                        if ' ' in part:\n",
        "                            url_part = part.split(' ', 1)[0]\n",
        "                        else:\n",
        "                            url_part = part\n",
        "                        if not url_part.startswith(('data:', 'javascript:', 'mailto:')):\n",
        "                            abs_url = urljoin(page_url, url_part)\n",
        "                            local_path = self.download_resource(abs_url, referer=page_url)\n",
        "                            if local_path:\n",
        "                                part = part.replace(url_part, local_path)\n",
        "                        urls.append(part)\n",
        "                    element[attr] = ', '.join(urls)\n",
        "                else:\n",
        "                    url = element.get(attr)\n",
        "                    if url and not url.startswith(('data:', 'javascript:', 'mailto:')):\n",
        "                        abs_url = urljoin(page_url, url)\n",
        "                        local_path = self.download_resource(abs_url, referer=page_url)\n",
        "                        if local_path:\n",
        "                            element[attr] = local_path\n",
        "\n",
        "        # Process inline styles\n",
        "        for element in soup.find_all(style=True):\n",
        "            style = element['style']\n",
        "            style = re.sub(\n",
        "                r'url\\([\\'\"]?(.*?)[\\'\"]?\\)',\n",
        "                lambda m: f\"url('{self.download_resource(urljoin(page_url, m.group(1)), page_url)}')\"\n",
        "                          if not m.group(1).startswith('data:')\n",
        "                          else m.group(0),\n",
        "                style\n",
        "            )\n",
        "            element['style'] = style\n",
        "\n",
        "        # Process CSS in style tags\n",
        "        for style_tag in soup.find_all('style'):\n",
        "            if style_tag.string:\n",
        "                style_tag.string = self.process_css(style_tag.string, page_url)\n",
        "\n",
        "        # Update links to other pages\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            href = a_tag['href']\n",
        "            if href and not href.startswith(('#', 'javascript:', 'mailto:')):\n",
        "                abs_url = urljoin(page_url, href)\n",
        "                if urlparse(abs_url).netloc == self.base_domain:\n",
        "                    a_tag['href'] = self.download_page(abs_url) or href\n",
        "\n",
        "        # Update form actions\n",
        "        for form_tag in soup.find_all('form', action=True):\n",
        "            action = form_tag['action']\n",
        "            if action and not action.startswith(('javascript:', 'mailto:')):\n",
        "                abs_url = urljoin(page_url, action)\n",
        "                if urlparse(abs_url).netloc == self.base_domain:\n",
        "                    form_tag['action'] = self.download_page(abs_url) or action\n",
        "\n",
        "        return str(soup)\n",
        "\n",
        "    def download_page(self, url):\n",
        "        \"\"\"Download and process a single HTML page\"\"\"\n",
        "        if url in self.visited_urls:\n",
        "            return self.asset_map.get(url)\n",
        "\n",
        "        self.visited_urls.add(url)\n",
        "        print(f\"Processing page: {url}\")\n",
        "\n",
        "        try:\n",
        "            # Handle relative URLs\n",
        "            if not urlparse(url).scheme:\n",
        "                url = urljoin(self.base_url, url)\n",
        "\n",
        "            response = self.session.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            content_type = response.headers.get('Content-Type', '').split(';')[0]\n",
        "            if not content_type or 'text/html' not in content_type:\n",
        "                return None\n",
        "\n",
        "            # Process HTML content\n",
        "            processed_html = self.process_html(response.content, url)\n",
        "\n",
        "            # Save HTML\n",
        "            local_relative_path = self.get_resource_path(url, 'text/html')\n",
        "            local_path = os.path.join(self.output_dir, local_relative_path)\n",
        "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "\n",
        "            with open(local_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(processed_html)\n",
        "\n",
        "            self.asset_map[url] = local_relative_path\n",
        "            return local_relative_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to process page {url}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Begin the download process\"\"\"\n",
        "        self.download_page(self.base_url)\n",
        "        print(f\"\\nWebsite successfully downloaded to: {self.output_dir}\")\n",
        "\n",
        "\n",
        "# Colab-compatible execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if running in Colab\n",
        "    if 'google.colab' in sys.modules:\n",
        "        print(\"Running in Google Colab\")\n",
        "        base_url = input(\"Enter website URL to download: \").strip()\n",
        "        output_dir = input(\"Enter output directory [downloaded_website]: \").strip() or \"downloaded_website\"\n",
        "    else:\n",
        "        # Command-line execution\n",
        "        import argparse\n",
        "        parser = argparse.ArgumentParser(description='Download website assets for offline use.')\n",
        "        parser.add_argument('url', help='URL of the website to download')\n",
        "        parser.add_argument('-o', '--output', default='downloaded_website',\n",
        "                            help='Output directory (default: downloaded_website)')\n",
        "        args = parser.parse_args()\n",
        "        base_url = args.url\n",
        "        output_dir = args.output\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Start downloader\n",
        "    downloader = WebsiteDownloader(base_url, output_dir)\n",
        "    downloader.start()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install playwright"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HzvbdBY1Ofk",
        "outputId": "4928fa79-5a8e-4243-ffd1-447ea2c69a10"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting playwright\n",
            "  Using cached playwright-1.53.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting pyee<14,>=13 (from playwright)\n",
            "  Downloading pyee-13.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.2.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<14,>=13->playwright) (4.14.1)\n",
            "Downloading playwright-1.53.0-py3-none-manylinux1_x86_64.whl (45.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-13.0.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyee, playwright\n",
            "Successfully installed playwright-1.53.0 pyee-13.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import mimetypes\n",
        "import hashlib\n",
        "import argparse\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from pathlib import Path\n",
        "from playwright.sync_api import sync_playwright\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "class AdvancedWebsiteDownloader:\n",
        "    def __init__(self, base_url, output_dir, depth=1, delay=1.0, max_retries=3):\n",
        "        self.base_url = base_url\n",
        "        parsed_url = urlparse(base_url)\n",
        "        self.base_domain = parsed_url.netloc\n",
        "        self.scheme = parsed_url.scheme or 'https'\n",
        "        self.output_dir = Path(output_dir).resolve()\n",
        "        self.depth = depth\n",
        "        self.delay = delay\n",
        "        self.max_retries = max_retries\n",
        "        self.visited_urls = set()\n",
        "        self.resource_map = {}\n",
        "        self.cookies = {}\n",
        "        self.user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.64 Safari/537.36\"\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': self.user_agent,\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Connection': 'keep-alive'\n",
        "        })\n",
        "        mimetypes.init()\n",
        "\n",
        "    def sanitize_path(self, path):\n",
        "        \"\"\"Create filesystem-safe paths from URLs with directory structure\"\"\"\n",
        "        if not path or path == '/':\n",
        "            return 'index.html'\n",
        "\n",
        "        path = re.sub(r'[^\\w\\-_\\.\\/]', '_', path)\n",
        "        path = path.strip('/')\n",
        "\n",
        "        # Handle extension-less paths\n",
        "        if '.' not in os.path.basename(path):\n",
        "            path += '.html'\n",
        "\n",
        "        return path\n",
        "\n",
        "    def get_local_path(self, url, content_type=None):\n",
        "        \"\"\"Generate local path while preserving directory structure\"\"\"\n",
        "        parsed = urlparse(url)\n",
        "        path = parsed.path or '/index.html'\n",
        "\n",
        "        # Handle root path\n",
        "        if path == '/':\n",
        "            return 'index.html'\n",
        "\n",
        "        # Handle directory paths\n",
        "        if path.endswith('/'):\n",
        "            path += 'index.html'\n",
        "\n",
        "        # Get filename\n",
        "        filename = os.path.basename(path)\n",
        "        dir_path = os.path.dirname(path)\n",
        "\n",
        "        # Add extension if missing\n",
        "        if '.' not in filename:\n",
        "            if content_type:\n",
        "                ext = mimetypes.guess_extension(content_type.split(';')[0]) or '.bin'\n",
        "                filename += ext\n",
        "            else:\n",
        "                # Try to guess from URL\n",
        "                ext = mimetypes.guess_extension(parsed.path.split('/')[-1]) or '.bin'\n",
        "                filename += ext\n",
        "\n",
        "        # Create domain directory\n",
        "        domain_dir = re.sub(r'[^\\w\\-_\\.]', '_', self.base_domain)\n",
        "\n",
        "        # Combine paths\n",
        "        return os.path.join(domain_dir, self.sanitize_path(dir_path), filename)\n",
        "\n",
        "    def download_resource(self, url, referer=None, is_retry=False):\n",
        "        \"\"\"Download resource with retry logic and proper content handling\"\"\"\n",
        "        if url in self.resource_map:\n",
        "            return self.resource_map[url]\n",
        "\n",
        "        # Skip data URLs\n",
        "        if url.startswith(('data:', 'javascript:', 'mailto:')):\n",
        "            return None\n",
        "\n",
        "        # Handle relative URLs\n",
        "        if not urlparse(url).scheme:\n",
        "            url = urljoin(f\"{self.scheme}://{self.base_domain}\", url)\n",
        "\n",
        "        try:\n",
        "            headers = {'Referer': referer} if referer else {}\n",
        "            response = self.session.get(url, headers=headers, timeout=15, stream=True)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Get content type\n",
        "            content_type = response.headers.get('Content-Type', '').split(';')[0]\n",
        "            if not content_type:\n",
        "                content_type = mimetypes.guess_type(url)[0] or 'application/octet-stream'\n",
        "\n",
        "            # Get local path\n",
        "            local_relative_path = self.get_local_path(url, content_type)\n",
        "            local_path = self.output_dir / local_relative_path\n",
        "\n",
        "            # Create directories\n",
        "            local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            # Handle text-based resources\n",
        "            if 'text/' in content_type or 'application/' in content_type:\n",
        "                content = response.content.decode('utf-8', errors='replace')\n",
        "\n",
        "                # Process based on content type\n",
        "                if 'css' in content_type:\n",
        "                    content = self.process_css(content, url)\n",
        "                elif 'html' in content_type:\n",
        "                    content = self.process_html(content, url)\n",
        "                elif 'javascript' in content_type:\n",
        "                    content = self.process_js(content, url)\n",
        "\n",
        "                with open(local_path, 'w', encoding='utf-8') as f:\n",
        "                    f.write(content)\n",
        "            else:\n",
        "                # Binary files\n",
        "                with open(local_path, 'wb') as f:\n",
        "                    for chunk in response.iter_content(8192):\n",
        "                        f.write(chunk)\n",
        "\n",
        "            print(f\"Downloaded: {url} => {local_relative_path}\")\n",
        "            self.resource_map[url] = local_relative_path\n",
        "            return local_relative_path\n",
        "\n",
        "        except Exception as e:\n",
        "            if not is_retry and self.max_retries > 0:\n",
        "                print(f\"Retrying ({self.max_retries} left): {url}\")\n",
        "                self.max_retries -= 1\n",
        "                time.sleep(1)\n",
        "                return self.download_resource(url, referer, is_retry=True)\n",
        "            print(f\"Failed to download {url}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def process_css(self, content, css_url):\n",
        "        \"\"\"Deep CSS processing including @import rules and nested URLs\"\"\"\n",
        "        # Process @import rules first\n",
        "        content = re.sub(\n",
        "            r'@import\\s+(url\\()?[\\'\"](.*?)[\\'\"]\\)?;',\n",
        "            lambda m: self.process_css_import(m, css_url),\n",
        "            content,\n",
        "            flags=re.IGNORECASE\n",
        "        )\n",
        "\n",
        "        # Process all url() references\n",
        "        return re.sub(\n",
        "            r'url\\([\\'\"]?(.*?)[\\'\"]?\\)',\n",
        "            lambda m: self.process_css_url(m, css_url),\n",
        "            content\n",
        "        )\n",
        "\n",
        "    def process_css_import(self, match, css_url):\n",
        "        \"\"\"Process CSS @import rules by downloading imported stylesheets\"\"\"\n",
        "        import_url = match.group(2).strip()\n",
        "        if import_url.startswith(('http:', 'https:')):\n",
        "            full_url = import_url\n",
        "        else:\n",
        "            full_url = urljoin(css_url, import_url)\n",
        "\n",
        "        local_path = self.download_resource(full_url, referer=css_url)\n",
        "        if local_path:\n",
        "            return f'@import url(\"{local_path}\");'\n",
        "        return match.group(0)\n",
        "\n",
        "    def process_css_url(self, match, css_url):\n",
        "        \"\"\"Process CSS url() references\"\"\"\n",
        "        url = match.group(1).strip('\\'\"')\n",
        "        if url.startswith(('data:', 'http:', 'https:')):\n",
        "            return match.group(0)\n",
        "\n",
        "        full_url = urljoin(css_url, url)\n",
        "        local_path = self.download_resource(full_url, referer=css_url)\n",
        "        return f'url(\"{local_path}\")' if local_path else match.group(0)\n",
        "\n",
        "    def process_js(self, content, js_url):\n",
        "        \"\"\"Process JavaScript files for dynamic resource loading\"\"\"\n",
        "        # Handle dynamic imports\n",
        "        content = re.sub(\n",
        "            r'import\\s*\\(?[\\'\"](.*?)[\\'\"]\\)?',\n",
        "            lambda m: self.process_js_import(m, js_url),\n",
        "            content\n",
        "        )\n",
        "\n",
        "        # Handle fetch/XHR calls (basic pattern matching)\n",
        "        content = re.sub(\n",
        "            r'fetch\\([\\'\"]((?!http).*?)[\\'\"]\\)',\n",
        "            lambda m: self.process_js_fetch(m, js_url),\n",
        "            content\n",
        "        )\n",
        "\n",
        "        return content\n",
        "\n",
        "    def process_js_import(self, match, js_url):\n",
        "        \"\"\"Process JavaScript dynamic imports\"\"\"\n",
        "        import_url = match.group(1).strip()\n",
        "        full_url = urljoin(js_url, import_url)\n",
        "        local_path = self.download_resource(full_url, referer=js_url)\n",
        "        return f'import(\"{local_path}\")' if local_path else match.group(0)\n",
        "\n",
        "    def process_js_fetch(self, match, js_url):\n",
        "        \"\"\"Process JavaScript fetch calls to relative URLs\"\"\"\n",
        "        fetch_url = match.group(1).strip()\n",
        "        full_url = urljoin(js_url, fetch_url)\n",
        "        local_path = self.download_resource(full_url, referer=js_url)\n",
        "        return f'fetch(\"{local_path}\")' if local_path else match.group(0)\n",
        "\n",
        "    def process_html(self, content, page_url):\n",
        "        \"\"\"Process HTML content with support for modern web features\"\"\"\n",
        "        soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "        # Process standard resource tags\n",
        "        resource_tags = [\n",
        "            ('link', 'href', lambda e: e.get('rel', [''])[0] not in ['alternate', 'canonical']),\n",
        "            ('script', 'src', None),\n",
        "            ('img', 'src', None),\n",
        "            ('img', 'srcset', None),\n",
        "            ('source', 'src', None),\n",
        "            ('source', 'srcset', None),\n",
        "            ('audio', 'src', None),\n",
        "            ('video', 'src', None),\n",
        "            ('video', 'poster', None),\n",
        "            ('iframe', 'src', None),\n",
        "            ('embed', 'src', None),\n",
        "            ('object', 'data', None),\n",
        "            ('meta', 'content', lambda e: 'property' in e.attrs and e['property'] in ['og:image', 'og:video']),\n",
        "            ('form', 'action', None),\n",
        "            ('button', 'formaction', None),\n",
        "            ('input', 'formaction', None),\n",
        "            ('a', 'ping', None),\n",
        "            ('area', 'href', None),\n",
        "        ]\n",
        "\n",
        "        for tag, attr, condition in resource_tags:\n",
        "            for element in soup.find_all(tag, **{attr: True}):\n",
        "                if condition and not condition(element):\n",
        "                    continue\n",
        "\n",
        "                urls = []\n",
        "                if attr == 'srcset':\n",
        "                    # Handle srcset with multiple URLs\n",
        "                    srcset = element[attr]\n",
        "                    parts = [p.strip() for p in srcset.split(',') if p.strip()]\n",
        "                    for part in parts:\n",
        "                        if ' ' in part:\n",
        "                            url_part, descriptor = part.split(' ', 1)\n",
        "                        else:\n",
        "                            url_part, descriptor = part, None\n",
        "\n",
        "                        if not url_part.startswith(('data:', 'javascript:')):\n",
        "                            full_url = urljoin(page_url, url_part)\n",
        "                            local_path = self.download_resource(full_url, referer=page_url)\n",
        "                            if local_path:\n",
        "                                new_part = local_path + (f' {descriptor}' if descriptor else '')\n",
        "                                urls.append(new_part)\n",
        "                            else:\n",
        "                                urls.append(part)\n",
        "                        else:\n",
        "                            urls.append(part)\n",
        "                    element[attr] = ', '.join(urls)\n",
        "                else:\n",
        "                    url = element.get(attr)\n",
        "                    if url and not url.startswith(('data:', 'javascript:')):\n",
        "                        full_url = urljoin(page_url, url)\n",
        "                        local_path = self.download_resource(full_url, referer=page_url)\n",
        "                        if local_path:\n",
        "                            element[attr] = local_path\n",
        "\n",
        "        # Process inline styles\n",
        "        for element in soup.find_all(style=True):\n",
        "            element['style'] = re.sub(\n",
        "                r'url\\([\\'\"]?(.*?)[\\'\"]?\\)',\n",
        "                lambda m: f\"url('{self.download_resource(urljoin(page_url, m.group(1))), page_url}')\"\n",
        "                          if not m.group(1).startswith('data:')\n",
        "                          else m.group(0),\n",
        "                element['style']\n",
        "            )\n",
        "\n",
        "        # Process inline scripts (basic)\n",
        "        for script in soup.find_all('script', string=True):\n",
        "            if script.string:\n",
        "                script.string = self.process_js(script.string, page_url)\n",
        "\n",
        "        # Process shadow DOM content\n",
        "        for template in soup.find_all('template'):\n",
        "            shadow_content = self.process_html(str(template), page_url)\n",
        "            template.clear()\n",
        "            template.append(BeautifulSoup(shadow_content, 'html.parser'))\n",
        "\n",
        "        # Process web components\n",
        "        for element in soup.find_all(attrs={\"shadowroot\": True}):\n",
        "            shadow_content = self.process_html(str(element), page_url)\n",
        "            element.clear()\n",
        "            element.append(BeautifulSoup(shadow_content, 'html.parser'))\n",
        "\n",
        "        # Process links to other pages\n",
        "        if self.depth > 0:\n",
        "            for a_tag in soup.find_all('a', href=True):\n",
        "                href = a_tag['href']\n",
        "                if href and not href.startswith(('#', 'javascript:', 'mailto:')):\n",
        "                    full_url = urljoin(page_url, href)\n",
        "                    if urlparse(full_url).netloc == self.base_domain and full_url not in self.visited_urls:\n",
        "                        self.download_page(full_url, depth=self.depth-1)\n",
        "\n",
        "        return str(soup)\n",
        "\n",
        "    def capture_with_playwright(self, url):\n",
        "        \"\"\"Use Playwright to render page and capture all network requests\"\"\"\n",
        "        with sync_playwright() as p:\n",
        "            browser = p.chromium.launch(headless=True)\n",
        "            context = browser.new_context(\n",
        "                user_agent=self.user_agent,\n",
        "                java_script_enabled=True,\n",
        "                ignore_https_errors=True\n",
        "            )\n",
        "\n",
        "            page = context.new_page()\n",
        "            page_resources = set()\n",
        "\n",
        "            def log_request(request):\n",
        "                page_resources.add(request.url)\n",
        "\n",
        "            page.on(\"request\", log_request)\n",
        "\n",
        "            try:\n",
        "                page.goto(url, wait_until=\"networkidle\", timeout=60000)\n",
        "                # Scroll to trigger lazy-loaded content\n",
        "                page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
        "                time.sleep(1)\n",
        "                content = page.content()\n",
        "                # Capture cookies for session\n",
        "                self.cookies = {c['name']: c['value'] for c in context.cookies()}\n",
        "                self.session.cookies.update(self.cookies)\n",
        "            finally:\n",
        "                context.close()\n",
        "                browser.close()\n",
        "\n",
        "            return content, page_resources\n",
        "\n",
        "    def download_page(self, url, depth=1):\n",
        "        \"\"\"Download a page with intelligent resource handling\"\"\"\n",
        "        if url in self.visited_urls:\n",
        "            return\n",
        "        self.visited_urls.add(url)\n",
        "\n",
        "        print(f\"Processing: {url} (depth: {depth})\")\n",
        "\n",
        "        try:\n",
        "            # Use Playwright to render page and capture resources\n",
        "            rendered_html, all_resources = self.capture_with_playwright(url)\n",
        "\n",
        "            # Download all resources captured by Playwright\n",
        "            for resource_url in all_resources:\n",
        "                if resource_url not in self.resource_map:\n",
        "                    self.download_resource(resource_url, referer=url)\n",
        "\n",
        "            # Process HTML after rendering\n",
        "            processed_html = self.process_html(rendered_html, url)\n",
        "\n",
        "            # Save main HTML\n",
        "            local_relative_path = self.get_local_path(url, 'text/html')\n",
        "            local_path = self.output_dir / local_relative_path\n",
        "            local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            with open(local_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(processed_html)\n",
        "\n",
        "            print(f\"Page saved: {local_relative_path}\")\n",
        "            return local_relative_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to process page {url}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the download process\"\"\"\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.download_page(self.base_url, depth=self.depth)\n",
        "\n",
        "        # Save resource map for debugging\n",
        "        with open(self.output_dir / 'resource_map.json', 'w') as f:\n",
        "            json.dump(self.resource_map, f, indent=2)\n",
        "\n",
        "        print(f\"\\nWebsite successfully downloaded to: {self.output_dir}\")\n",
        "        print(f\"Total resources downloaded: {len(self.resource_map)}\")\n",
        "        print(f\"Open {self.output_dir / self.get_local_path(self.base_url, 'text/html')} to view the site\")\n",
        "\n",
        "\n",
        "# Colab-compatible execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if running in Colab\n",
        "    if 'google.colab' in sys.modules:\n",
        "        print(\"Running in Google Colab\")\n",
        "        base_url = input(\"Enter website URL to download: \").strip()\n",
        "        output_dir = input(\"Enter output directory [downloaded_website]: \").strip() or \"downloaded_website\"\n",
        "        depth = int(input(\"Enter crawl depth [1]: \").strip() or 1)\n",
        "    else:\n",
        "        # Command-line execution\n",
        "        parser = argparse.ArgumentParser(description='Advanced Website Downloader')\n",
        "        parser.add_argument('url', help='URL of the website to download')\n",
        "        parser.add_argument('-o', '--output', default='downloaded_website',\n",
        "                           help='Output directory (default: downloaded_website)')\n",
        "        parser.add_argument('-d', '--depth', type=int, default=1,\n",
        "                           help='Crawl depth (default: 1)')\n",
        "        parser.add_argument('--delay', type=float, default=0.5,\n",
        "                           help='Delay between requests in seconds (default: 0.5)')\n",
        "        args = parser.parse_args()\n",
        "        base_url = args.url\n",
        "        output_dir = args.output\n",
        "        depth = args.depth\n",
        "\n",
        "    downloader = AdvancedWebsiteDownloader(\n",
        "        base_url=base_url,\n",
        "        output_dir=output_dir,\n",
        "        depth=depth,\n",
        "        delay=0.5\n",
        "    )\n",
        "    downloader.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMX3qpj1j4D_",
        "outputId": "90f1723e-2b20-4559-e86c-fd518a1e9c5a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab\n",
            "Enter website URL to download: https://theolivebranchpk.com/\n",
            "Enter output directory [downloaded_website]: /content/drive/MyDrive/PythonApps/WebsitesDownloader\n",
            "Enter crawl depth [1]: 2\n",
            "Processing: https://theolivebranchpk.com/ (depth: 2)\n",
            "Failed to process page https://theolivebranchpk.com/: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
            "Please use the Async API instead.\n",
            "\n",
            "Website successfully downloaded to: /content/drive/MyDrive/PythonApps/WebsitesDownloader\n",
            "Total resources downloaded: 0\n",
            "Open /content/drive/MyDrive/PythonApps/WebsitesDownloader/index.html to view the site\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zY-Y390P1gLe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}